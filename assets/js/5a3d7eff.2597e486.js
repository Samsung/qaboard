"use strict";(self.webpackChunkApache_2_0=self.webpackChunkApache_2_0||[]).push([[819],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return d}});var r=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var s=r.createContext({}),p=function(e){var t=r.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return r.createElement(s.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},m=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),m=p(n),d=a,g=m["".concat(s,".").concat(d)]||m[d]||u[d]||i;return n?r.createElement(g,o(o({ref:t},c),{},{components:n})):r.createElement(g,o({ref:t},c))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,o=new Array(i);o[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var p=2;p<i;p++)o[p]=n[p];return r.createElement.apply(null,o)}return r.createElement.apply(null,n)}m.displayName="MDXCreateElement"},4430:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return s},contentTitle:function(){return p},metadata:function(){return c},toc:function(){return u},default:function(){return d}});var r=n(7462),a=n(3366),i=(n(7294),n(3905)),o=n(4996),l=["components"],s={id:"deep-learning",sidebar_label:"Deep Learning & MLOps",title:"Deep Learning & MLOps Workflows"},p=void 0,c={unversionedId:"deep-learning",id:"deep-learning",title:"Deep Learning & MLOps Workflows",description:"Amazing tools exist for the R&D/training phase (like wandb.ai). They let you create easily create experiments without worrying about version control, without to setup a projects, and they have features that QA-Board doesn't have (compare N training curves at the same time, early stopping, nice reports built-in...).",source:"@site/docs/deep-learning.md",sourceDirName:".",slug:"/deep-learning",permalink:"/qaboard/docs/deep-learning",editUrl:"https://github.com/Samsung/qaboard/edit/master/website/docs/docs/deep-learning.md",tags:[],version:"current",frontMatter:{id:"deep-learning",sidebar_label:"Deep Learning & MLOps",title:"Deep Learning & MLOps Workflows"},sidebar:"docs",previous:{title:"CI Integration",permalink:"/qaboard/docs/ci-integration"},next:{title:"Debugging with IDEs",permalink:"/qaboard/docs/debugging-runs-with-an-IDE"}},u=[{value:"Workflows",id:"workflows",children:[{value:"What is needed to make this work?",id:"what-is-needed-to-make-this-work",children:[],level:3}],level:2},{value:"MLOps",id:"mlops",children:[],level:2}],m={toc:u};function d(e){var t=e.components,n=(0,a.Z)(e,l);return(0,i.kt)("wrapper",(0,r.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Amazing tools exist for the R&D/training phase (like ",(0,i.kt)("a",{parentName:"p",href:"https://wandb.ai/"},"wandb.ai"),"). They let you create easily create experiments without worrying about version control, without to setup a projects, and they have features that QA-Board doesn't have (compare N training curves at the same time, early stopping, nice reports built-in...)."),(0,i.kt)("p",null,"However, in our experience, tools built for ",(0,i.kt)("em",{parentName:"p"},"training")," usually lack important features to evaluate inference:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"going in depth into specific difficult inputs (in our case images...): the training loss doesn't tell the whole picture"),(0,i.kt)("li",{parentName:"ul"},"comparing versus reference/previous versions"),(0,i.kt)("li",{parentName:"ul"},"giving users tools to trigger runs on new inputs (e.g. test database supplied by client)"),(0,i.kt)("li",{parentName:"ul"},"integration with source control, CI systems, etc"),(0,i.kt)("li",{parentName:"ul"},"rich/custom visualization")),(0,i.kt)("p",null,"For instance here is how users can compare runs in QA-Board:"),(0,i.kt)("img",{alt:"wand experiments",src:(0,o.Z)("img/deep-learning/side-video.gif")}),(0,i.kt)("img",{alt:"wand experiments",src:(0,o.Z)("img/deep-learning/side-frames.gif")}),(0,i.kt)("p",null,"So how do we use QA-Board and deep learning tools? Well, we setup our projects to have them work well together."),(0,i.kt)("h2",{id:"workflows"},"Workflows"),(0,i.kt)("p",null,"Since training is best done with dedicated tools, our projects are setup to show in QA-Board:"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"the evaluation metrics"),(0,i.kt)("li",{parentName:"ul"},"model parameters and training summary"),(0,i.kt)("li",{parentName:"ul"},"link to the training page")),(0,i.kt)("img",{alt:"wand experiments",src:(0,o.Z)("img/deep-learning/qa-wand-infoa.png")}),(0,i.kt)("p",null,"In practice:"),(0,i.kt)("img",{alt:"wand experiments",src:(0,o.Z)("img/deep-learning/qa-link-wandb.gif")}),(0,i.kt)("p",null,"Users get the best of both worlds: QA-Board, and the experiement/training-loss UI from ",(0,i.kt)("inlineCode",{parentName:"p"},"wandb"),"..."),(0,i.kt)("img",{alt:"wand experiments",src:(0,o.Z)("img/deep-learning/wandb-experiments.png")}),(0,i.kt)("img",{alt:"wand experiments",src:(0,o.Z)("img/deep-learning/wandb-metrics.png")}),(0,i.kt)("h3",{id:"what-is-needed-to-make-this-work"},"What is needed to make this work?"),(0,i.kt)("p",null,"Users will write a ",(0,i.kt)("inlineCode",{parentName:"p"},"run()")," function that:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Reads from ",(0,i.kt)("inlineCode",{parentName:"li"},"context.params")," some model ID (or defaults to the model that was trained for the commit)"),(0,i.kt)("li",{parentName:"ol"},"Fetches details about that model from some API: hyper-params, link to the training page... It will return those ",(0,i.kt)("a",{parentName:"li",href:"computing-quantitative-metrics#metrics-shown-as-a-run-time-configuration"},"as badges")),(0,i.kt)("li",{parentName:"ol"},"Runs the model on the ",(0,i.kt)("inlineCode",{parentName:"li"},"context.input_path")),(0,i.kt)("li",{parentName:"ol"},"Does postprocessing to compute various evaluation metrics/plots")),(0,i.kt)("img",{alt:"wand experiments",src:(0,o.Z)("img/deep-learning/filter.gif")}),(0,i.kt)("div",{className:"admonition admonition-tip alert alert--success"},(0,i.kt)("div",{parentName:"div",className:"admonition-heading"},(0,i.kt)("h5",{parentName:"div"},(0,i.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,i.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},(0,i.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"tip")),(0,i.kt)("div",{parentName:"div",className:"admonition-content"},(0,i.kt)("p",{parentName:"div"},"Some engineers have a script that filters/sort their recent models according to various hyperparams/metrics, and start evaluation with QA-Board... Ideally we should be able to trigger directly inferences from the training page but not all ML tools make it easy!"))),(0,i.kt)("h2",{id:"mlops"},"MLOps"),(0,i.kt)("p",null,"When using MLops flows, users will have the CI:"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Train")," a network"),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Fail")," if the training loss is too high"),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Run inferences")," on an evaluation batch using QA-Board"),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("strong",{parentName:"li"},"Fail")," if the evaluation metrics are bad")))}d.isMDXComponent=!0}}]);